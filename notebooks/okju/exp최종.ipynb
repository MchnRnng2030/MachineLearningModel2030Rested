{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d3cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ì²­ë…„ ë…¸ë™ì‹œì¥ ì •ì±… ìŠ¤í¬ë¦¬ë‹ ëª¨ë¸ - FINAL COMPLETE VERSION\n",
    "\n",
    "ğŸ¯ ëª©ì \n",
    "- ê°œì¸ì˜ 'ë…¸ë ¥/ì„ íƒ'ì´ ì•„ë‹Œ êµ¬ì¡°ì  ìœ„ì¹˜(structural position)ì— ê¸°ë°˜í•˜ì—¬\n",
    "  ì²­ë…„ì˜ ë…¸ë™ì‹œì¥ ë‹¨ì ˆ ìœ„í—˜ê³¼ ì¬ì—°ê²° ê°€ëŠ¥ì„±ì„ ì§„ë‹¨\n",
    "- í–‰ì • íŒë‹¨ ìë™í™” X (ê²°ì •ê¶Œ ëŒ€ì²´ ì•„ë‹˜)\n",
    "- ì •ì±… ëŒ€ìƒì 1ì°¨ ì„ ë³„(Screening) ã…‡\n",
    "- ì •ì±… ìì› ë°°ë¶„ íš¨ìœ¨ì„±(Lift) ê²€ì¦ ã…‡\n",
    "- ì„¤ëª… ê°€ëŠ¥ì„±(Explainability)ê³¼ ì •ì±… ì¬í˜„ì„± í™•ë³´\n",
    "\n",
    "í•µì‹¬ ì² í•™\n",
    "- \"ì˜ˆì¸¡ ëª¨ë¸\"ì´ ì•„ë‹ˆë¼\n",
    "- \"ì •ì±… ë¶„ë¥˜ ë„êµ¬ + ìœ„í—˜ í™˜ê²½ ì§„ë‹¨ ë„êµ¬\"\n",
    "\n",
    "ğŸ“¦ ì „ì²´ êµ¬ì¡°\n",
    "PART 0. ë°ì´í„° ë¡œë“œ ë° ìƒê´€ê´€ê³„ ë¶„ì„ (ì •ë³´ëˆ„ìˆ˜ ì œê±°)\n",
    "PART 1. êµ¬ì¡°ì  ë‹¨ì ˆìœ„í—˜ ëª¨ë¸ (í™˜ê²½ ìœ„í—˜, GBDT)\n",
    "PART 2. ê°œì¸ ì·¨ì—…í™•ë¥  ëª¨ë¸ (ê°œì¸ ìƒíƒœ, GBDT + Calibration)\n",
    "PART 3. ì •ì±… ë§¤íŠ¸ë¦­ìŠ¤ ê²°í•© (í™˜ê²½ Ã— ê°œì¸)\n",
    "PART 4. EDA ì‹œê°í™” (êµ¬ì¡° ì´í•´)\n",
    "PART 5. ë°ì´í„° ê¸°ë°˜ í˜ë¥´ì†Œë‚˜ (ì •ì±… ì–¸ì–´í™”)\n",
    "PART 6. ì •ì±… íƒ€ê²ŸíŒ… ì„±ëŠ¥ í‰ê°€ + ì‹œë®¬ë ˆì´ì…˜\n",
    "\"\"\"\n",
    "\n",
    "# ======================================================\n",
    "# 0ï¸âƒ£ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í™˜ê²½ ì„¤ì •\n",
    "# ======================================================\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# íŒŒì¼ ë° ê²½ë¡œ ê´€ë¦¬\n",
    "# ------------------------------------------------------\n",
    "import glob      # ë‹¤ìˆ˜ íŒŒì¼ íŒ¨í„´ ë¡œë”© (ì—°ë„ë³„ CSV)\n",
    "import os        # ë””ë ‰í† ë¦¬ ìƒì„± ë° ê²½ë¡œ ê´€ë¦¬\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# ê²½ê³  ì œê±° (ëª¨ë¸ ë¡œê·¸ ê°€ë…ì„± í–¥ìƒ)\n",
    "# ------------------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# ìˆ˜ì¹˜ ê³„ì‚° ë° ë°ì´í„° ì²˜ë¦¬\n",
    "# ------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# ì‹œê°í™”\n",
    "# ------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# ë¨¸ì‹ ëŸ¬ë‹\n",
    "# ------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,      # ì˜ˆì¸¡ë ¥ (ìˆœìœ„ ê¸°ë°˜)\n",
    "    recall_score,       # ì •ì±… í¬ì°©ë¥ \n",
    "    precision_score     # ì˜¤íƒ ë°©ì§€\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "# ------------------------------------------------------\n",
    "import joblib\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# í•œê¸€ ì‹œê°í™” ì„¤ì •\n",
    "# ------------------------------------------------------\n",
    "plt.rcParams[\"font.family\"] = \"Gulim\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# ğŸ“ ê²°ê³¼ ì €ì¥ ê²½ë¡œ\n",
    "# ======================================================\n",
    "\n",
    "\"\"\"\n",
    "ëª¨ë¸ ê²°ê³¼ëŠ” ì„¸ ì˜ì—­ìœ¼ë¡œ ë¶„ë¦¬ ì €ì¥\n",
    "- models   : í•™ìŠµëœ ëª¨ë¸ ê°ì²´ (ì¬í˜„ì„±)\n",
    "- results  : ì‹œê°í™” ê²°ê³¼ (ë³´ê³ ì„œ)\n",
    "- data     : ì •ì±… ë§¤íŠ¸ë¦­ìŠ¤ ìµœì¢… ê²°ê³¼\n",
    "\"\"\"\n",
    "\n",
    "MODEL_JSON_DIR = \"../../results/oxju\"        # ì‹œê°í™” ê²°ê³¼\n",
    "MODEL_DIR      = \"../../models/oxju\"         # ëª¨ë¸ ê°ì²´\n",
    "CSV_DIR        = \"../../data/derived/oxju\"   # ìµœì¢… ì •ì±… ë°ì´í„°\n",
    "\n",
    "# ê²½ë¡œê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±\n",
    "os.makedirs(MODEL_JSON_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(CSV_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# ğŸ”§ íŒŒìƒë³€ìˆ˜ ìƒì„± í•¨ìˆ˜\n",
    "# ======================================================\n",
    "def add_derived_features(df):\n",
    "    \"\"\"\n",
    "    ëª©ì \n",
    "    - ë¹„ì„ í˜•ì„± ë³´ì™„\n",
    "    - ì •ì±… í•´ì„ì´ ê°€ëŠ¥í•œ êµ¬ì¡°ì  íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
    "    - ë‹¨ìˆœ ì˜ˆì¸¡ë ¥ì´ ì•„ë‹Œ 'ì„¤ëª… ê°€ëŠ¥í•œ ëª¨ë¸' í™•ë³´\n",
    "\n",
    "    ì„¤ê³„ ì›ì¹™\n",
    "    - ê°œì¸ ì„ íƒ/í–‰ë™ ë³€ìˆ˜ X\n",
    "    - êµ¬ì¡°ì  ìœ„ì¹˜Â·ì†ì„± ë³€ìˆ˜ ã…‡\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # -------------------------------\n",
    "    # ì—°ë ¹ëŒ€ ë”ë¯¸ (ë¹„ì„ í˜• êµ¬ì¡° ëŒ€ì‘)\n",
    "    # -------------------------------\n",
    "    df[\"ì—°ë ¹_20ëŒ€ì´ˆë°˜\"] = (df[\"ë§Œì—°ë ¹\"] <= 24).astype(int)\n",
    "    df[\"ì—°ë ¹_20ëŒ€í›„ë°˜\"] = ((df[\"ë§Œì—°ë ¹\"] >= 25) & (df[\"ë§Œì—°ë ¹\"] <= 29)).astype(int)\n",
    "    df[\"ì—°ë ¹_30ëŒ€ì´ˆë°˜\"] = ((df[\"ë§Œì—°ë ¹\"] >= 30) & (df[\"ë§Œì—°ë ¹\"] <= 34)).astype(int)\n",
    "    df[\"ì—°ë ¹_30ëŒ€í›„ë°˜\"] = (df[\"ë§Œì—°ë ¹\"] >= 35).astype(int)\n",
    "\n",
    "    # -------------------------------\n",
    "    # ì„±ë³„ Ã— í˜¼ì¸ ìƒíƒœ (êµ¬ì¡°ì  ìœ„ì¹˜)\n",
    "    # -------------------------------\n",
    "    df[\"ì—¬ì„±ê¸°í˜¼\"] = ((df[\"ì„±ë³„ì½”ë“œ\"] == 2) & (df[\"í˜¼ì¸ìƒíƒœì½”ë“œ\"] == 2)).astype(int)\n",
    "    df[\"ë‚¨ì„±ë¯¸í˜¼\"] = ((df[\"ì„±ë³„ì½”ë“œ\"] == 1) & (df[\"í˜¼ì¸ìƒíƒœì½”ë“œ\"] == 1)).astype(int)\n",
    "    df[\"ì—¬ì„±ë¯¸í˜¼\"] = ((df[\"ì„±ë³„ì½”ë“œ\"] == 2) & (df[\"í˜¼ì¸ìƒíƒœì½”ë“œ\"] == 1)).astype(int)\n",
    "\n",
    "    # -------------------------------\n",
    "    # í•™ë ¥ êµ¬ì¡°\n",
    "    # -------------------------------\n",
    "    df[\"ê³ ì¡¸ì´í•˜\"] = (df[\"êµìœ¡ì •ë„_í•™ë ¥êµ¬ë¶„ì½”ë“œ\"] <= 3).astype(int)\n",
    "    df[\"ì „ë¬¸ëŒ€ì¡¸\"] = (df[\"êµìœ¡ì •ë„_í•™ë ¥êµ¬ë¶„ì½”ë“œ\"] == 4).astype(int)\n",
    "    df[\"ëŒ€ì¡¸ì´ìƒ\"] = (df[\"êµìœ¡ì •ë„_í•™ë ¥êµ¬ë¶„ì½”ë“œ\"] >= 5).astype(int)\n",
    "\n",
    "    # -------------------------------\n",
    "    # ì „ê³µ ê³„ì—´ êµ¬ì¡°\n",
    "    # -------------------------------\n",
    "    df[\"ì´ê³µê³„\"] = df[\"êµìœ¡ì •ë„_ê³„ì—´ì½”ë“œ\"].isin([3, 50, 60, 70, 80]).astype(int)\n",
    "    df[\"ì¸ë¬¸ì‚¬íšŒ\"] = df[\"êµìœ¡ì •ë„_ê³„ì—´ì½”ë“œ\"].isin([1, 22, 30, 40]).astype(int)\n",
    "\n",
    "    # -------------------------------\n",
    "    # ì—°ë ¹ ë¹„ì„ í˜•ì„±\n",
    "    # -------------------------------\n",
    "    df[\"ë§Œì—°ë ¹_ì œê³±\"] = df[\"ë§Œì—°ë ¹\"] ** 2\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# PART 0 : ì²­ë…„ ë¯¸ì‹œ ë°ì´í„° ë¡œë“œ\n",
    "# ======================================================\n",
    "def load_youth_may_full(path=\"../../data/youth_may/*.csv\", sample_files=3):\n",
    "    \"\"\"\n",
    "    ëª©ì \n",
    "    - ê°œì¸ ë‹¨ìœ„ ë¯¸ì‹œ ë°ì´í„° ë¡œë“œ\n",
    "    - ì·¨ì—… ì—¬ë¶€ ë° 'ì§„ì„± ì‰¬ì—ˆìŒ' ìƒíƒœ ì •ì˜\n",
    "    - ì·¨ì—…í™•ë¥  ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ìƒì„±\n",
    "\n",
    "    ì •ì±…ì  ì˜ë¯¸\n",
    "    - 'ë…¸ë ¥ ë¶€ì¡±'ì´ ì•„ë‹Œ ìƒíƒœ(state) ê¸°ë°˜ ë¶„ë¥˜\n",
    "    - ì§„ì„± ì‰¬ì—ˆìŒ = êµ¬ì¡°ì ìœ¼ë¡œ ë…¸ë™ì‹œì¥ê³¼ ë‹¨ì ˆëœ ìƒíƒœ\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ”¬ [ì²­ë…„ ë¯¸ì‹œ ë°ì´í„° ë¡œë“œ]\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # ì¼ë¶€ ì—°ë„ë§Œ ìƒ˜í”Œ ë¡œë”© (ê°œë°œ/ê²€ì¦ ëª©ì )\n",
    "    files = glob.glob(path)[:sample_files]\n",
    "    dfs = []\n",
    "\n",
    "    for f in files:\n",
    "        df_tmp = pd.read_csv(f)\n",
    "        dfs.append(df_tmp)\n",
    "\n",
    "    # ë‹¤ë…„ ë°ì´í„° ë³‘í•©\n",
    "    df_full = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # ë¶„ì„ ëŒ€ìƒ ì—°ë ¹ ì œí•œ\n",
    "    df_full = df_full[df_full[\"ë§Œì—°ë ¹\"].between(20, 39)].copy()\n",
    "\n",
    "    print(f\"âˆš ë¡œë“œ ì™„ë£Œ: {len(df_full):,}ëª…\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # ì·¨ì—… ì—¬ë¶€ (íƒ€ê²Ÿ)\n",
    "    # -------------------------------\n",
    "    df_full[\"ì·¨ì—…ì—¬ë¶€\"] = (df_full[\"ê²½ì œí™œë™ìƒíƒœì½”ë“œ\"] == 1).astype(int)\n",
    "\n",
    "    # -------------------------------\n",
    "    # ì§„ì„± ì‰¬ì—ˆìŒ ì •ì˜\n",
    "    # -------------------------------\n",
    "    cond_no_search = df_full[\"ê¸°íƒ€í™œë™ì‚¬í•­_ì§€ë‚œ1ë…„ë‚´êµ¬ì§í™œë™ìœ ë¬´\"] == 2\n",
    "    cond_labor_reason = df_full[\"ê¸°íƒ€í™œë™ì‚¬í•­_4ì£¼ë‚´ë¹„êµ¬ì§ì‚¬ìœ ì½”ë“œ\"].isin([1,2,3,4,5,6])\n",
    "\n",
    "    df_full[\"ì§„ì„±ì‰¬ì—ˆìŒ\"] = (cond_no_search & cond_labor_reason).astype(int)\n",
    "\n",
    "    # -------------------------------\n",
    "    # ì—°ë ¹ëŒ€ ê·¸ë£¹\n",
    "    # -------------------------------\n",
    "    df_full[\"ì—°ë ¹ëŒ€\"] = pd.cut(\n",
    "        df_full[\"ë§Œì—°ë ¹\"],\n",
    "        [19, 24, 29, 34, 39],\n",
    "        labels=[\"20-24\", \"25-29\", \"30-34\", \"35-39\"]\n",
    "    )\n",
    "\n",
    "    return df_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 0 (í›„ë°˜): ìƒê´€ê´€ê³„ ë¶„ì„ & ì •ë³´ëˆ„ìˆ˜ ì œê±°\n",
    "def analyze_full_correlation(df, target=\"ì§„ì„±ì‰¬ì—ˆìŒ\", top_n=30):\n",
    "    \"\"\"\n",
    "    ëª©ì \n",
    "    - íƒ€ê²Ÿ(ì§„ì„± ì‰¬ì—ˆìŒ)ê³¼ í†µê³„ì ìœ¼ë¡œ ì—°ê´€ëœ ë³€ìˆ˜ íƒìƒ‰\n",
    "    - ì‚¬í›„ ì •ë³´Â·í–‰ìœ„ ë³€ìˆ˜ë¡œ ì¸í•œ 'ì •ë³´ ëˆ„ìˆ˜' ì‚¬ì „ ì°¨ë‹¨\n",
    "\n",
    "    ì •ì±…ì  ì˜ë¯¸\n",
    "    - ì´ ë‹¨ê³„ëŠ” 'ë³€ìˆ˜ ì„ íƒ'ì´ ì•„ë‹ˆë¼\n",
    "      ! ì •ì±…ì  ì •ë‹¹ì„± ê²€ì¦ ë‹¨ê³„\n",
    "    - ì´ ë‹¨ê³„ë¥¼ ê±°ì¹˜ì§€ ì•Šìœ¼ë©´\n",
    "      â†’ ëª¨ë¸ì´ ì˜ˆì¸¡ì´ ì•„ë‹Œ 'íŒì •ê¸°'ê°€ ë¨\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ğŸ”¥ [{target}ê³¼ ìƒê´€ê´€ê³„ ë¶„ì„]\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 1. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ ì¶”ì¶œ\n",
    "    # ----------------------------------------\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 2. ê²°ì¸¡ì¹˜ ê³¼ë‹¤ ë³€ìˆ˜ ì œê±°\n",
    "    # ----------------------------------------\n",
    "    valid_cols = [\n",
    "        col for col in numeric_cols\n",
    "        if df[col].notna().sum() > len(df) * 0.3\n",
    "    ]\n",
    "\n",
    "    print(f\"ğŸ“Š ë¶„ì„ ëŒ€ìƒ ë³€ìˆ˜ ìˆ˜: {len(valid_cols)}\")\n",
    "\n",
    "    # ìƒê´€ê³„ìˆ˜ ê³„ì‚°ìš© ë°ì´í„°\n",
    "    df_corr = df[valid_cols].dropna()\n",
    "\n",
    "    # ë°ì´í„° ë¶€ì¡± ì‹œ ì¤‘ë‹¨\n",
    "    if len(df_corr) < 100:\n",
    "        return None, []\n",
    "\n",
    "    corr_matrix = df_corr.corr()\n",
    "\n",
    "    if target not in corr_matrix.columns:\n",
    "        return corr_matrix, []\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 3. íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê³„ìˆ˜ ì ˆëŒ€ê°’ ê¸°ì¤€ ì •ë ¬\n",
    "    # ----------------------------------------\n",
    "    target_corr = corr_matrix[target].abs().sort_values(ascending=False)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 4. ì‹œê°í™” (í•„í„°ë§ ì „)\n",
    "    # ----------------------------------------\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    top_vars = target_corr.drop(target).head(top_n)\n",
    "\n",
    "    # ìƒê´€ 0.1 ì´ìƒ ê°•ì¡°\n",
    "    colors = ['#ff6b6b' if x > 0.1 else '#95a5a6' for x in top_vars.values]\n",
    "\n",
    "    plt.barh(range(len(top_vars)), top_vars.values, color=colors)\n",
    "    plt.yticks(range(len(top_vars)), top_vars.index, fontsize=9)\n",
    "    plt.xlabel(\"ìƒê´€ê³„ìˆ˜ (ì ˆëŒ€ê°’)\")\n",
    "    plt.title(f\"'{target}'ê³¼ ìƒê´€ê´€ê³„ TOP {top_n} (í•„í„°ë§ ì „)\")\n",
    "    plt.axvline(x=0.1, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{MODEL_JSON_DIR}/correlation_bar_before_filter.png\", dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 5. ì •ë³´ëˆ„ìˆ˜ ë¸”ë™ë¦¬ìŠ¤íŠ¸\n",
    "    # ----------------------------------------\n",
    "    \"\"\"\n",
    "    X ì œê±° ëŒ€ìƒ\n",
    "    - êµ¬ì§ ì—¬ë¶€\n",
    "    - í˜„ì¬ ìƒíƒœ\n",
    "    - ì‚¬í›„ ì •ë³´\n",
    "    - ì •ì±… ê°œì… ì´í›„ ì•Œ ìˆ˜ ìˆëŠ” ì •ë³´\n",
    "    \"\"\"\n",
    "\n",
    "    BLACKLIST = [\n",
    "        \"ì§„ì„±ì‰¬ì—ˆìŒ\", \"ì·¨ì—…ì—¬ë¶€\",\n",
    "        \"ê¸°íƒ€í™œë™ì‚¬í•­_ì§€ë‚œ1ë…„ë‚´êµ¬ì§í™œë™ìœ ë¬´\",\n",
    "        \"ê¸°íƒ€í™œë™ì‚¬í•­_4ì£¼ë‚´ë¹„êµ¬ì§ì‚¬ìœ ì½”ë“œ\",\n",
    "        \"ê²½ì œí™œë™ìƒíƒœ\", \"ì£¼ìš”í™œë™ìƒíƒœ\",\n",
    "        \"ë¯¸ì·¨ì—…\", \"ì „ì§\", \"í‡´ì§\", \"ì´ì§\", \"ì´ì „ì§ì¥\",\n",
    "        \"í˜„ì¬ì¼ê´€ë ¨\", \"ì¼ì‹œíœ´ì§\", \"ê·¼ë¡œì‹œê°„\", \n",
    "        \"ë¶€ì—…\", \"ë¬´ê¸‰ê°€ì¡±\", \"ì¢…ì‚¬ìƒì§€ìœ„\",\n",
    "        \"êµ¬ì§ì‚¬í•­\", \"êµ¬ì§ì—¬ë¶€\",\n",
    "        \"ì·¨ì—…í¬ë§\", \"ì·¨ì—…ê°€ëŠ¥ì„±\",\n",
    "        \"ì¡°ì‚¬ì—°ì›”\", \"ê°€ì¤‘ì¹˜\",\n",
    "    ]\n",
    "\n",
    "    high_corr_vars = []\n",
    "    excluded_count = 0\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 6. ì•ˆì „ ë³€ìˆ˜ ì„ ë³„\n",
    "    # ----------------------------------------\n",
    "    for var in target_corr.index:\n",
    "        if var == target:\n",
    "            continue\n",
    "\n",
    "        # ë¸”ë™ë¦¬ìŠ¤íŠ¸ í•„í„°\n",
    "        if any(keyword in var for keyword in BLACKLIST):\n",
    "            excluded_count += 1\n",
    "            continue\n",
    "\n",
    "        # ê²°ì¸¡ë¥  ê³¼ë‹¤ ì œê±°\n",
    "        if df[var].isna().sum() / len(df) > 0.7:\n",
    "            excluded_count += 1\n",
    "            continue\n",
    "\n",
    "        # ìµœì†Œ ìƒê´€ ê¸°ì¤€\n",
    "        if target_corr[var] >= 0.02:\n",
    "            high_corr_vars.append(var)\n",
    "\n",
    "    print(\"\\nâˆš í•„í„°ë§ ê²°ê³¼\")\n",
    "    print(f\"   - ì œì™¸ ë³€ìˆ˜: {excluded_count}\")\n",
    "    print(f\"   - ì‚¬ìš© ê°€ëŠ¥ ë³€ìˆ˜: {len(high_corr_vars)}\")\n",
    "\n",
    "    return corr_matrix, high_corr_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91293d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1: êµ¬ì¡°ì  ë‹¨ì ˆìœ„í—˜ ëª¨ë¸ (í™˜ê²½ ìœ„í—˜)\n",
    "def load_eaps_year(path=\"../../data/eaps_year/*.csv\"):\n",
    "    \"\"\"\n",
    "    ëª©ì \n",
    "    - ê°œì¸ì´ ì•„ë‹Œ 'ë…¸ë™ì‹œì¥ í™˜ê²½ ìœ„í—˜' ìƒì„±\n",
    "    - ì§€ì—­Â·ì—°ë ¹Â·ì„±ë³„ êµ¬ì¡°ì  ë‹¨ì ˆ í™•ë¥  ì¶”ì •\n",
    "\n",
    "    í•µì‹¬ ê°œë…\n",
    "    - ê°œì¸ ì±…ì„ X\n",
    "    - í™˜ê²½ ë¦¬ìŠ¤í¬ ã…‡\n",
    "    \"\"\"\n",
    "\n",
    "    USE_COLS = [\n",
    "        \"ì¡°ì‚¬ì—°ì›”\", \"ë§Œì—°ë ¹\", \"ì„±ë³„ì½”ë“œ\",\n",
    "        \"êµìœ¡ì •ë„_í•™ë ¥êµ¬ë¶„ì½”ë“œ\", \"êµìœ¡ì •ë„_ê³„ì—´ì½”ë“œ\",\n",
    "        \"í˜¼ì¸ìƒíƒœì½”ë“œ\", \"ê²½ì œí™œë™ìƒíƒœì½”ë“œ\",\n",
    "        \"ë¹„ê²½ì œí™œë™êµ¬ì§ì˜ì‚¬ì—¬ë¶€\"\n",
    "    ]\n",
    "\n",
    "    dfs = [pd.read_csv(f, usecols=USE_COLS) for f in glob.glob(path)]\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    df = df[df[\"ë§Œì—°ë ¹\"].between(20, 39)]\n",
    "\n",
    "    # ì—°ë ¹ëŒ€ ìƒì„±\n",
    "    df[\"ì—°ë ¹ëŒ€\"] = pd.cut(\n",
    "        df[\"ë§Œì—°ë ¹\"], [19, 24, 29, 34, 39],\n",
    "        labels=[\"20-24\", \"25-29\", \"30-34\", \"35-39\"]\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # êµ¬ì¡°ì  ë‹¨ì ˆ ì •ì˜\n",
    "    # ----------------------------------------\n",
    "    df[\"ì‹œì¥ë‹¨ì ˆ\"] = (\n",
    "        (df[\"ê²½ì œí™œë™ìƒíƒœì½”ë“œ\"] == 3) &\n",
    "        (df[\"ë¹„ê²½ì œí™œë™êµ¬ì§ì˜ì‚¬ì—¬ë¶€\"] == 2)\n",
    "    ).astype(int)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_disconnect_model_enhanced(df):\n",
    "    \"\"\"\n",
    "    GBDT ê¸°ë°˜ êµ¬ì¡°ì  ë‹¨ì ˆ ìœ„í—˜ ëª¨ë¸\n",
    "\n",
    "    ì´ìœ \n",
    "    - ë¹„ì„ í˜• êµ¬ì¡° ëŒ€ì‘\n",
    "    - ë³€ìˆ˜ ìƒí˜¸ì‘ìš© ìë™ í¬ì°©\n",
    "    \"\"\"\n",
    "\n",
    "    df = add_derived_features(df)\n",
    "\n",
    "    FEATURES = [\n",
    "        \"ì„±ë³„ì½”ë“œ\", \"ë§Œì—°ë ¹\", \"ë§Œì—°ë ¹_ì œê³±\",\n",
    "        \"êµìœ¡ì •ë„_í•™ë ¥êµ¬ë¶„ì½”ë“œ\", \"êµìœ¡ì •ë„_ê³„ì—´ì½”ë“œ\",\n",
    "        \"í˜¼ì¸ìƒíƒœì½”ë“œ\",\n",
    "        \"ì—¬ì„±ê¸°í˜¼\", \"ë‚¨ì„±ë¯¸í˜¼\", \"ì—¬ì„±ë¯¸í˜¼\",\n",
    "        \"ê³ ì¡¸ì´í•˜\", \"ëŒ€ì¡¸ì´ìƒ\",\n",
    "        \"ì—°ë ¹_20ëŒ€ì´ˆë°˜\", \"ì—°ë ¹_20ëŒ€í›„ë°˜\",\n",
    "        \"ì—°ë ¹_30ëŒ€ì´ˆë°˜\", \"ì—°ë ¹_30ëŒ€í›„ë°˜\"\n",
    "    ]\n",
    "\n",
    "    # ëª¨ë¸ ì…ë ¥ êµ¬ì„±\n",
    "    dfm = df[FEATURES + [\"ì‹œì¥ë‹¨ì ˆ\"]].dropna()\n",
    "\n",
    "    X = pd.get_dummies(dfm[FEATURES], drop_first=True)\n",
    "    y = dfm[\"ì‹œì¥ë‹¨ì ˆ\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # GBDT ëª¨ë¸\n",
    "    # ----------------------------------------\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        min_samples_leaf=50,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(\"\\n[êµ¬ì¡°ì  ë‹¨ì ˆ ìœ„í—˜ ëª¨ë¸]\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "\n",
    "    # ì „ì²´ ë°ì´í„°ì— ìœ„í—˜ í™•ë¥  ë¶€ì—¬\n",
    "    X_all = pd.get_dummies(df[FEATURES], drop_first=True)\n",
    "    X_all = X_all.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "    df[\"ë‹¨ì ˆìœ„í—˜í™•ë¥ \"] = model.predict_proba(X_all)[:, 1]\n",
    "\n",
    "    return (\n",
    "        model,\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        df\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2: ê°œì¸ ì·¨ì—…í™•ë¥  ëª¨ë¸ (Calibration í¬í•¨)\n",
    "def train_employment_gbdt_enhanced(df, additional_features=[]):\n",
    "    \"\"\"\n",
    "    ëª©ì \n",
    "    - ê°œì¸ì˜ êµ¬ì¡°ì  ìœ„ì¹˜ ê¸°ë°˜ ì·¨ì—… ê°€ëŠ¥ì„± ì¶”ì •\n",
    "    - í™•ë¥ ì˜ 'í•´ì„ ê°€ëŠ¥ì„±' í™•ë³´ (Calibration)\n",
    "\n",
    "    í•µì‹¬\n",
    "    - ë…¸ë ¥/ì˜ì§€ ë³€ìˆ˜ X\n",
    "    - êµ¬ì¡°ì  ìƒíƒœ ë³€ìˆ˜ ã…‡\n",
    "    \"\"\"\n",
    "\n",
    "    df = add_derived_features(df)\n",
    "\n",
    "    BASE_FEATURES = [\n",
    "        \"ì„±ë³„ì½”ë“œ\", \"ë§Œì—°ë ¹\", \"ë§Œì—°ë ¹_ì œê³±\",\n",
    "        \"êµìœ¡ì •ë„_í•™ë ¥êµ¬ë¶„ì½”ë“œ\", \"êµìœ¡ì •ë„_ê³„ì—´ì½”ë“œ\",\n",
    "        \"í˜¼ì¸ìƒíƒœì½”ë“œ\",\n",
    "        \"ì—°ë ¹_20ëŒ€ì´ˆë°˜\", \"ì—°ë ¹_20ëŒ€í›„ë°˜\",\n",
    "        \"ì—°ë ¹_30ëŒ€ì´ˆë°˜\", \"ì—°ë ¹_30ëŒ€í›„ë°˜\",\n",
    "        \"ì—¬ì„±ê¸°í˜¼\", \"ë‚¨ì„±ë¯¸í˜¼\", \"ì—¬ì„±ë¯¸í˜¼\",\n",
    "        \"ê³ ì¡¸ì´í•˜\", \"ì „ë¬¸ëŒ€ì¡¸\", \"ëŒ€ì¡¸ì´ìƒ\",\n",
    "        \"ì´ê³µê³„\", \"ì¸ë¬¸ì‚¬íšŒ\"\n",
    "    ]\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # ìƒê´€ë¶„ì„ì—ì„œ ì¶”ì¶œëœ ì•ˆì „ ë³€ìˆ˜ ì¶”ê°€\n",
    "    # ----------------------------------------\n",
    "    NEW_FEATURES = []\n",
    "    for f in additional_features[:15]:\n",
    "        if f in BASE_FEATURES:\n",
    "            continue\n",
    "        if f not in df.columns:\n",
    "            continue\n",
    "        if not pd.api.types.is_numeric_dtype(df[f]):\n",
    "            continue\n",
    "        if df[f].isna().sum() / len(df) > 0.7:\n",
    "            continue\n",
    "        NEW_FEATURES.append(f)\n",
    "\n",
    "    FEATURES = BASE_FEATURES + NEW_FEATURES\n",
    "\n",
    "    print(f\"\\nâˆš ì·¨ì—… ëª¨ë¸ ë³€ìˆ˜ ìˆ˜: {len(FEATURES)}\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # í•™ìŠµ ë°ì´í„° êµ¬ì„±\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    print(f\"   - ê¸°ë³¸: {len(BASE_FEATURES)}ê°œ, ì¶”ê°€: {len(NEW_FEATURES)}ê°œ\")\n",
    "    if NEW_FEATURES:\n",
    "        print(f\"   - ì¶”ê°€ëœ ë³€ìˆ˜:\")\n",
    "        for i, v in enumerate(NEW_FEATURES, 1):\n",
    "            print(f\"      {i}. {v}\")\n",
    "    \n",
    "    keep_cols = [\n",
    "        \"ì¡°ì‚¬ì—°ì›”\", \"ì—°ë ¹ëŒ€\", \"ì§„ì„±ì‰¬ì—ˆìŒ\", \n",
    "        \"ì„±ë³„ì½”ë“œ\", \"ë§Œì—°ë ¹\", \"êµìœ¡ì •ë„_í•™ë ¥êµ¬ë¶„ì½”ë“œ\", \"ì·¨ì—…ì—¬ë¶€\"\n",
    "    ]\n",
    "    \n",
    "    all_cols = list(dict.fromkeys(FEATURES + keep_cols))\n",
    "    \n",
    "    dfm = df[all_cols].dropna()\n",
    "    \n",
    "    train_df, test_df = train_test_split(\n",
    "        dfm, test_size=0.2, stratify=dfm[\"ì·¨ì—…ì—¬ë¶€\"], random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train = pd.get_dummies(train_df[FEATURES], drop_first=True)\n",
    "    X_test = pd.get_dummies(test_df[FEATURES], drop_first=True)\n",
    "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "    \n",
    "    y_train = train_df[\"ì·¨ì—…ì—¬ë¶€\"]\n",
    "    y_test = test_df[\"ì·¨ì—…ì—¬ë¶€\"]\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # GBDT + Isotonic Calibration\n",
    "    # ----------------------------------------\n",
    "    base = GradientBoostingClassifier(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        min_samples_leaf=30,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model = CalibratedClassifierCV(base, method=\"isotonic\", cv=3)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(\"\\n[ì·¨ì—… í™•ë¥  ëª¨ë¸]\")\n",
    "    print(f\"AUC       : {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "    print(f\"Recall    : {recall_score(y_test, y_prob >= 0.5):.4f}\")\n",
    "    print(f\"Precision : {precision_score(y_test, y_prob >= 0.5):.4f}\")\n",
    "\n",
    "    test_df = test_df.copy()\n",
    "    test_df[\"ì·¨ì—…ì˜ˆì¸¡í™•ë¥ \"] = y_prob\n",
    "\n",
    "    return (\n",
    "        model,\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        test_df\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3: ì •ì±… ë§¤íŠ¸ë¦­ìŠ¤ ê²°í•© (í™˜ê²½ Ã— ê°œì¸)\n",
    "def build_hybrid_policy_matrix(df_main, df_test):\n",
    "    \"\"\"\n",
    "    ëª©ì \n",
    "    - PART 1(í™˜ê²½ ìœ„í—˜) + PART 2(ê°œì¸ ì·¨ì—…í™•ë¥ )ì„ ê²°í•©í•˜ì—¬\n",
    "      ì •ì±… ëŒ€ìƒì ìŠ¤í¬ë¦¬ë‹ì„ ìœ„í•œ 'ì •ì±… ë§¤íŠ¸ë¦­ìŠ¤'ë¥¼ ìƒì„±\n",
    "\n",
    "    í•µì‹¬ ì•„ì´ë””ì–´ (ì´ ëª¨ë¸ì˜ ì •ì²´ì„±)\n",
    "    - ê°œì¸ ë‹¨ìœ„ í™•ë¥ ë§Œ ë³´ë©´ 'ê°œì¸ ì±…ì„'ìœ¼ë¡œ í•´ì„ë  ìœ„í—˜ì´ ìˆìŒ\n",
    "    - í™˜ê²½ ë‹¨ìœ„ ìœ„í—˜ë§Œ ë³´ë©´ 'ê°œì¸ ë‹¤ì–‘ì„±'ì„ ë†“ì¹¨\n",
    "\n",
    "    ë”°ë¼ì„œ\n",
    "    - í™˜ê²½ ìœ„í—˜(êµ¬ì¡°) Ã— ê°œì¸ ì·¨ì—…í™•ë¥ (ê°œì¸)ì„ ê²°í•©í•œ 2ì°¨ì› ë¶„ë¥˜ê°€ í•µì‹¬\n",
    "\n",
    "    ê²°ê³¼ì ìœ¼ë¡œ ì •ì±…êµ°ì€ 4ê°œë¡œ ë‚˜ë‰œë‹¤:\n",
    "    1) ê³ ì°©í˜• ë‹¨ì ˆ ìœ„í—˜êµ°       : í™˜ê²½ ìœ„í—˜ â†‘, ê°œì¸ ì·¨ì—…í™•ë¥  â†“\n",
    "    2) ì¬ì—°ê²° ê°€ëŠ¥ ë‹¨ì ˆêµ°       : í™˜ê²½ ìœ„í—˜ â†‘, ê°œì¸ ì·¨ì—…í™•ë¥  â†‘\n",
    "    3) ì·¨ì—…ì·¨ì•½ ë¹„ë‹¨ì ˆêµ°        : í™˜ê²½ ìœ„í—˜ â†“, ê°œì¸ ì·¨ì—…í™•ë¥  â†“\n",
    "    4) ì•ˆì •êµ°                  : í™˜ê²½ ìœ„í—˜ â†“, ê°œì¸ ì·¨ì—…í™•ë¥  â†‘\n",
    "\n",
    "    ì¶œë ¥\n",
    "    - df_policy : ê°œì¸ ë‹¨ìœ„ ì •ì±…ìœ í˜• + ì¬ì—°ê²°ì§€ìˆ˜ í¬í•¨\n",
    "    - risk_env  : (ì¡°ì‚¬ì—°ì›”Ã—ì„±ë³„Ã—ì—°ë ¹ëŒ€) í™˜ê²½ ìœ„í—˜ í…Œì´ë¸”\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 1. í™˜ê²½ ìœ„í—˜ì„ ë§Œë“¤ê¸° ìœ„í•œ KEY ì •ì˜\n",
    "    # ----------------------------------------\n",
    "    # ë™ì¼í•œ êµ¬ì¡°ì  í™˜ê²½ì„ ê³µìœ í•˜ëŠ” ì§‘ë‹¨ ë‹¨ìœ„\n",
    "    # (ì—°ì›”, ì„±ë³„, ì—°ë ¹ëŒ€) ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í•‘\n",
    "    KEYS = [\"ì¡°ì‚¬ì—°ì›”\", \"ì„±ë³„ì½”ë“œ\", \"ì—°ë ¹ëŒ€\"]\n",
    "\n",
    "    # í˜¼í•©í˜• mergeë¥¼ ìœ„í•´ íƒ€ì… í†µì¼\n",
    "    df_main[\"ì¡°ì‚¬ì—°ì›”\"] = df_main[\"ì¡°ì‚¬ì—°ì›”\"].astype(str)\n",
    "    df_test[\"ì¡°ì‚¬ì—°ì›”\"] = df_test[\"ì¡°ì‚¬ì—°ì›”\"].astype(str)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 2. í™˜ê²½ ìœ„í—˜(ë‹¨ì ˆìœ„í—˜_í™˜ê²½í‰ê· ) ê³„ì‚°\n",
    "    # ----------------------------------------\n",
    "    # PART 1ì˜ ê°œì¸ë³„ ë‹¨ì ˆìœ„í—˜í™•ë¥ ì„\n",
    "    # í™˜ê²½ ë‹¨ìœ„(ì—°ì›”Ã—ì„±ë³„Ã—ì—°ë ¹ëŒ€) í‰ê· ìœ¼ë¡œ ì§‘ê³„\n",
    "    risk_env = (\n",
    "        df_main\n",
    "        .groupby(KEYS)\n",
    "        .agg(ë‹¨ì ˆìœ„í—˜_í™˜ê²½í‰ê· =(\"ë‹¨ì ˆìœ„í—˜í™•ë¥ \", \"mean\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    risk_env[\"ì¡°ì‚¬ì—°ì›”\"] = risk_env[\"ì¡°ì‚¬ì—°ì›”\"].astype(str)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 3. ê°œì¸ ë°ì´í„°(df_test)ì— í™˜ê²½ ìœ„í—˜ merge\n",
    "    # ----------------------------------------\n",
    "    df_policy = df_test.merge(risk_env, on=KEYS, how=\"left\")\n",
    "\n",
    "    # ë§Œì•½ ë§¤ì¹­ ì‹¤íŒ¨(í•´ë‹¹ í™˜ê²½ ì¡°í•©ì´ ì—†ì„ ë•Œ) -> ì „ì²´ í‰ê· ìœ¼ë¡œ ëŒ€ì²´\n",
    "    # ì •ì±… ì ìš©ì‹œ \"í™˜ê²½ì •ë³´ê°€ ì—†ëŠ” ê°œì¸\"ì„ ë²„ë¦¬ë©´ ì»¤ë²„ë¦¬ì§€ ì†ìƒ\n",
    "    df_policy[\"ë‹¨ì ˆìœ„í—˜_í™˜ê²½í‰ê· \"].fillna(\n",
    "        df_main[\"ë‹¨ì ˆìœ„í—˜í™•ë¥ \"].mean(), inplace=True\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 4. ì—°ë ¹ëŒ€ë³„ ì»·ì˜¤í”„(quantile) ì„¤ì •\n",
    "    # ----------------------------------------\n",
    "    \"\"\"\n",
    "    ì™œ ì—°ë ¹ëŒ€ë³„ë¡œ ì»·ì˜¤í”„ë¥¼ ë”°ë¡œ ì¡ë‚˜?\n",
    "    - 20-24, 35-39ì˜ ê¸°ë³¸ ì·¨ì—… í™•ë¥  ë¶„í¬ ìì²´ê°€ ë‹¤ë¦„\n",
    "    - í•˜ë‚˜ì˜ ê³µí†µ ì„ê³„ê°’(ì˜ˆ: 0.5)ì„ ì“°ë©´ ì—°ë ¹ í¸í–¥ì´ ìƒê¹€\n",
    "    - ë”°ë¼ì„œ ì—°ë ¹ëŒ€ ë‚´ë¶€ì—ì„œ ìƒëŒ€ì  ìœ„ì¹˜ë¡œ ë¶„ë¥˜\n",
    "\n",
    "    RISK_Q: í™˜ê²½ ìœ„í—˜ì˜ ìƒìœ„ ëª‡ %ë¥¼ \"ìœ„í—˜êµ°\"ìœ¼ë¡œ ë³¼ì§€\n",
    "    JOB_Q : ì·¨ì—…í™•ë¥ ì˜ í•˜ìœ„ ëª‡ %ë¥¼ \"ì·¨ì—…ì·¨ì•½\"ìœ¼ë¡œ ë³¼ì§€\n",
    "    \"\"\"\n",
    "    # ğŸ”¥ ì§‘ì¤‘ íƒ€ê²ŸíŒ…ìš© (Lift ìƒìŠ¹)\n",
    "    RISK_Q = {\"20-24\": 0.75, \"25-29\": 0.75, \"30-34\": 0.80, \"35-39\": 0.80}\n",
    "    JOB_Q  = {\"20-24\": 0.30, \"25-29\": 0.30, \"30-34\": 0.35, \"35-39\": 0.35}\n",
    "\n",
    "    # ì •ì±…ìœ í˜• ì»¬ëŸ¼ ì´ˆê¸°í™”\n",
    "    df_policy[\"ì •ì±…ìœ í˜•\"] = None\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 5. ì—°ë ¹ëŒ€ë³„ë¡œ ìœ„í—˜/ì·¨ì—… ì»·ì„ êµ¬í•´ 4ë¶„ë©´ ë¶„ë¥˜\n",
    "    # ----------------------------------------\n",
    "    for age in [\"20-24\", \"25-29\", \"30-34\", \"35-39\"]:\n",
    "        age_mask = df_policy[\"ì—°ë ¹ëŒ€\"] == age\n",
    "        age_data = df_policy[age_mask]\n",
    "\n",
    "        # ìƒ˜í”Œì´ ë„ˆë¬´ ì ìœ¼ë©´ ë¶„ìœ„ìˆ˜ ì»· ìì²´ê°€ ë¶ˆì•ˆì • -> ìŠ¤í‚µ\n",
    "        if len(age_data) < 30:\n",
    "            continue\n",
    "\n",
    "        # í™˜ê²½ ìœ„í—˜ ì»·: ìƒìœ„ risk_q ì´ìƒì´ë©´ \"ìœ„í—˜ í™˜ê²½\"\n",
    "        risk_cut = age_data[\"ë‹¨ì ˆìœ„í—˜_í™˜ê²½í‰ê· \"].quantile(RISK_Q[age])\n",
    "\n",
    "        # ì·¨ì—… í™•ë¥  ì»·: í•˜ìœ„ job_q ë¯¸ë§Œì´ë©´ \"ì·¨ì—… ì·¨ì•½\"\n",
    "        job_cut  = age_data[\"ì·¨ì—…ì˜ˆì¸¡í™•ë¥ \"].quantile(JOB_Q[age])\n",
    "\n",
    "        # np.selectë¡œ 4ë¶„ë©´ ë¶„ë¥˜\n",
    "        df_policy.loc[age_mask, \"ì •ì±…ìœ í˜•\"] = np.select(\n",
    "            [\n",
    "                # 1) ìœ„í—˜ í™˜ê²½ & ì·¨ì—… ì·¨ì•½\n",
    "                (age_data[\"ë‹¨ì ˆìœ„í—˜_í™˜ê²½í‰ê· \"] >= risk_cut) &\n",
    "                (age_data[\"ì·¨ì—…ì˜ˆì¸¡í™•ë¥ \"] < job_cut),\n",
    "\n",
    "                # 2) ìœ„í—˜ í™˜ê²½ & ì·¨ì—… ê°€ëŠ¥\n",
    "                (age_data[\"ë‹¨ì ˆìœ„í—˜_í™˜ê²½í‰ê· \"] >= risk_cut) &\n",
    "                (age_data[\"ì·¨ì—…ì˜ˆì¸¡í™•ë¥ \"] >= job_cut),\n",
    "\n",
    "                # 3) ì•ˆì • í™˜ê²½ & ì·¨ì—… ì·¨ì•½\n",
    "                (age_data[\"ë‹¨ì ˆìœ„í—˜_í™˜ê²½í‰ê· \"] < risk_cut) &\n",
    "                (age_data[\"ì·¨ì—…ì˜ˆì¸¡í™•ë¥ \"] < job_cut),\n",
    "            ],\n",
    "            [\"ê³ ì°©í˜• ë‹¨ì ˆ ìœ„í—˜êµ°\", \"ì¬ì—°ê²° ê°€ëŠ¥ ë‹¨ì ˆêµ°\", \"ì·¨ì—…ì·¨ì•½ ë¹„ë‹¨ì ˆêµ°\"],\n",
    "            default=\"ì•ˆì •êµ°\"\n",
    "        )\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 6. ì¬ì—°ê²°ì§€ìˆ˜ ì •ì˜ (ë‹¨ì¼ ìŠ¤ì½”ì–´)\n",
    "    # ----------------------------------------\n",
    "    \"\"\"\n",
    "    ì¬ì—°ê²°ì§€ìˆ˜(Reconnection Index)\n",
    "    - ì •ì±… ì§‘í–‰ ì‹¤ë¬´ì—ì„œ 2ì°¨ì› ë§¤íŠ¸ë¦­ìŠ¤ëŠ” ì§ê´€ì ì´ì§€ë§Œ\n",
    "      'ìš°ì„ ìˆœìœ„'ë¥¼ ì •í•  ë• ë‹¨ì¼ ì ìˆ˜ê°€ ë” í•„ìš”í•¨\n",
    "\n",
    "    ì„¤ê³„ ì² í•™\n",
    "    - ì¬ì—°ê²° ê°€ëŠ¥ì„± = (í™˜ê²½ì˜ ì•ˆì „í•¨) + (ê°œì¸ì˜ ì·¨ì—… ê°€ëŠ¥ì„±)\n",
    "    - ë‹¨ì ˆìœ„í—˜ì€ ë‚®ì„ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ (1 - ìœ„í—˜)\n",
    "    - í˜„ì¬ ê°€ì¤‘ì¹˜ëŠ” 0.3 / 0.7 ë¡œ ê°œì¸ì·¨ì—…í™•ë¥ ì„ ë” í¬ê²Œ ë°˜ì˜\n",
    "      â†’ \"í™˜ê²½ì´ ë‚˜ì˜ë”ë¼ë„ ê°œì¸ íšŒë³µ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©´ ì§€ì› íš¨ìœ¨ì´ ë†’ë‹¤\"\n",
    "      ë¼ëŠ” ì •ì±…ì  ê°€ì •ì— ê¸°ë°˜\n",
    "\n",
    "    â€» ê°€ì¤‘ì¹˜ ìì²´ëŠ” ì •ì±…ëª©ì ì— ë”°ë¼ ë°”ë€” ìˆ˜ ìˆìŒ\n",
    "    \"\"\"\n",
    "    df_policy[\"ì¬ì—°ê²°ì§€ìˆ˜\"] = (\n",
    "        0.3 * (1 - df_policy[\"ë‹¨ì ˆìœ„í—˜_í™˜ê²½í‰ê· \"]) +\n",
    "        0.7 * df_policy[\"ì·¨ì—…ì˜ˆì¸¡í™•ë¥ \"]\n",
    "    ).round(3)\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # 7. ì§‘ì¤‘ íƒ€ê²ŸíŒ…ìš© ì§€ì› í•„ìš”ë„ ì ìˆ˜\n",
    "    # ----------------------------------------\n",
    "    \"\"\"\n",
    "    ì§€ì›í•„ìš”ë„ì ìˆ˜ (Need Score)\n",
    "    - ìœ„í—˜ì´ ë†’ê³ \n",
    "    - ì·¨ì—…í™•ë¥ ì´ ë‚®ì„ìˆ˜ë¡\n",
    "    - ì ìˆ˜ê°€ ë†’ì•„ì§\n",
    "    \"\"\"\n",
    "    df_policy[\"ì§€ì›í•„ìš”ë„ì ìˆ˜\"] = (\n",
    "        0.7 * df_policy[\"ë‹¨ì ˆìœ„í—˜_í™˜ê²½í‰ê· \"] +\n",
    "        0.3 * (1 - df_policy[\"ì·¨ì—…ì˜ˆì¸¡í™•ë¥ \"])\n",
    "    ).round(4)\n",
    "\n",
    "\n",
    "    return df_policy, risk_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 4: EDA ì‹œê°í™” (ì •ì±…êµ°ì˜ êµ¬ì¡° í™•ì¸)\n",
    "def plot_eda_visualizations(df_policy):\n",
    "    \"\"\"\n",
    "    ëª©ì \n",
    "    - ì •ì±… ìœ í˜•ë³„ ë¶„í¬/êµ¬ì¡°ë¥¼ ì§ê´€ì ìœ¼ë¡œ í™•ì¸\n",
    "    - ì •ì±… ëŒ€ìƒìì˜ â€œì–´ë–¤ íŠ¹ì„±ì˜ ì§‘ë‹¨ì´ ì–´ë””ì— ëª°ë¦¬ëŠ”ì§€â€ ì„¤ëª…ìë£Œ ìƒì„±\n",
    "\n",
    "    ìƒì„± ê·¸ë˜í”„\n",
    "    1) Ridge Plot        : ì •ì±…ìœ í˜•ë³„ ì—°ë ¹ ë¶„í¬ ì°¨ì´\n",
    "    2) Scatter Plot      : (ì·¨ì—…í™•ë¥ , í™˜ê²½ìœ„í—˜) 2ì°¨ì› ì •ì±… ë¶„í¬\n",
    "    3) Boxplot           : ì •ì±…ìœ í˜•ë³„ ì¬ì—°ê²°ì§€ìˆ˜ ë¶„í¬\n",
    "    4) Targeting Barplot : ì •ì±…ìœ í˜•ë³„ ì§„ì„±ì‰¬ì—ˆìŒ ë¹„ìœ¨(Lift ì§ê´€)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"â€» [EDA ì‹œê°í™” ìƒì„±]\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 1) Ridge Plot (ì—¬ê¸°ì„œëŠ” KDE ê³¡ì„ ì„ ê²¹ì³ í‘œí˜„)\n",
    "    # --------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # ì •ì±…ìœ í˜•ë³„ë¡œ KDEë¥¼ ê²¹ì³ ê·¸ë ¤ ë¶„í¬ ë¹„êµ\n",
    "    for group in df_policy[\"ì •ì±…ìœ í˜•\"].unique():\n",
    "        subset = df_policy[df_policy[\"ì •ì±…ìœ í˜•\"] == group]\n",
    "\n",
    "        # í‘œë³¸ì´ ì ìœ¼ë©´ KDEê°€ ë¶ˆì•ˆì • -> ìµœì†Œ ê°œìˆ˜ ì œí•œ\n",
    "        if len(subset) > 10:\n",
    "            subset[\"ë§Œì—°ë ¹\"].plot(kind='kde', ax=ax, label=group, alpha=0.6)\n",
    "\n",
    "    ax.set_xlabel(\"ë§Œì—°ë ¹\")\n",
    "    ax.set_title(\"ì •ì±… ìœ í˜•ë³„ ì—°ë ¹ ë¶„í¬\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{MODEL_JSON_DIR}/ridge_plot_final.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 2) Cluster Scatter (ì •ì±…ìœ í˜•ì´ ì‹¤ì œë¡œ ë¶„ë¦¬ë˜ëŠ”ì§€ í™•ì¸)\n",
    "    # --------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    for group in df_policy[\"ì •ì±…ìœ í˜•\"].unique():\n",
    "        subset = df_policy[df_policy[\"ì •ì±…ìœ í˜•\"] == group]\n",
    "        ax.scatter(\n",
    "            subset[\"ì·¨ì—…ì˜ˆì¸¡í™•ë¥ \"],\n",
    "            subset[\"ë‹¨ì ˆìœ„í—˜_í™˜ê²½í‰ê· \"],\n",
    "            label=group,\n",
    "            alpha=0.6,\n",
    "            s=30\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"ì·¨ì—…ì˜ˆì¸¡í™•ë¥ \")\n",
    "    ax.set_ylabel(\"ë‹¨ì ˆìœ„í—˜(í™˜ê²½í‰ê· )\")\n",
    "    ax.set_title(\"ì •ì±… í´ëŸ¬ìŠ¤í„° ë¶„í¬ (í™•ë¥ Ã—í™˜ê²½)\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{MODEL_JSON_DIR}/cluster_scatter.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 3) Boxplot (ì •ì±…ìœ í˜•ë³„ ì¬ì—°ê²°ì§€ìˆ˜ ë¶„í¬)\n",
    "    # --------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # pandas ë‚´ì¥ boxplot ì‚¬ìš©\n",
    "    df_policy.boxplot(column=\"ì¬ì—°ê²°ì§€ìˆ˜\", by=\"ì •ì±…ìœ í˜•\", ax=ax)\n",
    "\n",
    "    ax.set_title(\"ì •ì±… ìœ í˜•ë³„ ì¬ì—°ê²° ì§€ìˆ˜ ë¶„í¬\")\n",
    "    ax.set_xlabel(\"ì •ì±…ìœ í˜•\")\n",
    "    ax.set_ylabel(\"ì¬ì—°ê²°ì§€ìˆ˜\")\n",
    "\n",
    "    # pandas boxplotì€ ê¸°ë³¸ì ìœ¼ë¡œ suptitleì´ ë“¤ì–´ê°€ë¯€ë¡œ ì œê±°\n",
    "    plt.suptitle(\"\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{MODEL_JSON_DIR}/boxplot_reconnect.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 4) Targeting ì„±ëŠ¥ (ì •ì±…ìœ í˜•ë³„ ì§„ì„±ì‰¬ì—ˆìŒ ë¹„ìœ¨)\n",
    "    # --------------------------------------------------\n",
    "    # ì •ì±…êµ°ë³„ 'ì§„ì„±ì‰¬ì—ˆìŒ' í‰ê·  = í•´ë‹¹ ì •ì±…êµ°ì´ ì‹¤ì œ íƒ€ê²Ÿì„ ì–¼ë§ˆë‚˜ í¬í•¨í•˜ëŠ”ì§€\n",
    "    segment_stats = df_policy.groupby(\"ì •ì±…ìœ í˜•\")[\"ì§„ì„±ì‰¬ì—ˆìŒ\"].agg([\"mean\", \"count\"])\n",
    "    segment_stats[\"mean\"] = segment_stats[\"mean\"] * 100\n",
    "    segment_stats = segment_stats.sort_values(\"mean\", ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # ìœ„í—˜ë„ê°€ 10% ì´ìƒì´ë©´ ê°•ì¡° ìƒ‰ìƒ ë¶€ì—¬\n",
    "    colors = ['#e74c3c' if x > 10 else '#95a5a6' for x in segment_stats[\"mean\"]]\n",
    "\n",
    "    ax.barh(range(len(segment_stats)), segment_stats[\"mean\"], color=colors)\n",
    "    ax.set_yticks(range(len(segment_stats)))\n",
    "    ax.set_yticklabels(segment_stats.index)\n",
    "    ax.set_xlabel(\"ì§„ì„± ì‰¬ì—ˆìŒ ë¹„ìœ¨ (%)\")\n",
    "    ax.set_title(\"ì •ì±… ìœ í˜•ë³„ íƒ€ê²ŸíŒ… ì„±ëŠ¥(ì§„ì„± ì‰¬ì—ˆìŒ í¬ì°©ë¥ )\")\n",
    "\n",
    "    # ì „ì²´ í‰ê· ì„  (ì •ì±…êµ°ì´ í‰ê· ë³´ë‹¤ ì–¼ë§ˆë‚˜ ë†’ì€ì§€ ì§ê´€ ì œê³µ)\n",
    "    ax.axvline(\n",
    "        x=df_policy[\"ì§„ì„±ì‰¬ì—ˆìŒ\"].mean() * 100,\n",
    "        color='red', linestyle='--', label='ì „ì²´ í‰ê· '\n",
    "    )\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{MODEL_JSON_DIR}/targeting_accuracy.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"ì‹œê°í™” ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b1269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 5: ë°ì´í„° ê¸°ë°˜ í˜ë¥´ì†Œë‚˜ (ì •ì±… ì–¸ì–´ë¡œ ë³€í™˜)\n",
    "def generate_personas(df_policy):\n",
    "    \"\"\"\n",
    "    ëª©ì \n",
    "    - ì •ì±… ë§¤íŠ¸ë¦­ìŠ¤ì˜ 4ê°œ êµ°ì§‘ì„\n",
    "      'ì •ì±… ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ê°€ëŠ¥í•œ í˜ë¥´ì†Œë‚˜'ë¡œ ìš”ì•½\n",
    "\n",
    "    ì´ìœ \n",
    "    - ì •ì±…ì€ ìˆ«ìë§Œìœ¼ë¡œ ì„¤ë“ë˜ì§€ ì•ŠìŒ\n",
    "    - ì •ì±…ìœ í˜•ë³„ ëŒ€í‘œ íŠ¹ì„±ì„ ìš”ì•½í•´ì•¼\n",
    "      ì‹¤ë¬´ì/ì˜ì‚¬ê²°ì •ìê°€ ì´í•´Â·ì„¤ëª… ê°€ëŠ¥\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ­ [ë°ì´í„° ê¸°ë°˜ í˜ë¥´ì†Œë‚˜]\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # ì •ì±…ìœ í˜•ì„ ê³ ì • ìˆœì„œë¡œ ì¶œë ¥ (ë³´ê³ ì„œ ì •ë ¬ìš©)\n",
    "    for group in [\"ê³ ì°©í˜• ë‹¨ì ˆ ìœ„í—˜êµ°\", \"ì¬ì—°ê²° ê°€ëŠ¥ ë‹¨ì ˆêµ°\", \"ì·¨ì—…ì·¨ì•½ ë¹„ë‹¨ì ˆêµ°\", \"ì•ˆì •êµ°\"]:\n",
    "        subset = df_policy[df_policy[\"ì •ì±…ìœ í˜•\"] == group]\n",
    "\n",
    "        # í•´ë‹¹ ê·¸ë£¹ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ\n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "\n",
    "        # ----------------------------------------\n",
    "        # í˜ë¥´ì†Œë‚˜ í•µì‹¬ ìš”ì•½ í†µê³„\n",
    "        # ----------------------------------------\n",
    "        avg_age = subset[\"ë§Œì—°ë ¹\"].mean()\n",
    "\n",
    "        # ìµœë¹ˆê°’ ê¸°ë°˜ ëŒ€í‘œ ì„±ë³„\n",
    "        gender_mode = subset[\"ì„±ë³„ì½”ë“œ\"].mode()[0] if len(subset[\"ì„±ë³„ì½”ë“œ\"].mode()) > 0 else 1\n",
    "        gender_str = \"ì—¬ì„±\" if gender_mode == 2 else \"ë‚¨ì„±\"\n",
    "\n",
    "        # ìµœë¹ˆê°’ ê¸°ë°˜ ëŒ€í‘œ í•™ë ¥\n",
    "        edu_mode = subset[\"êµìœ¡ì •ë„_í•™ë ¥êµ¬ë¶„ì½”ë“œ\"].mode()[0] if len(subset[\"êµìœ¡ì •ë„_í•™ë ¥êµ¬ë¶„ì½”ë“œ\"].mode()) > 0 else 5\n",
    "        edu_map = {1: \"ì¤‘ì¡¸ì´í•˜\", 2: \"ê³ ì¡¸\", 3: \"ê³ ì¡¸\", 4: \"ì „ë¬¸ëŒ€ì¡¸\", 5: \"ëŒ€ì¡¸\", 6: \"ëŒ€ì¡¸ì´ìƒ\"}\n",
    "        edu_str = edu_map.get(edu_mode, \"ëŒ€ì¡¸\")\n",
    "\n",
    "        # ì§„ë‹¨ ì§€í‘œë“¤\n",
    "        emp_rate      = subset[\"ì·¨ì—…ì—¬ë¶€\"].mean() * 100\n",
    "        risk_avg      = subset[\"ë‹¨ì ˆìœ„í—˜_í™˜ê²½í‰ê· \"].mean() * 100\n",
    "        reconnect_avg = subset[\"ì¬ì—°ê²°ì§€ìˆ˜\"].mean()\n",
    "        target_rate   = subset[\"ì§„ì„±ì‰¬ì—ˆìŒ\"].mean() * 100\n",
    "\n",
    "        # ----------------------------------------\n",
    "        # ì¶œë ¥ (ë³´ê³ ì„œ ë³µë¶™ìš©)\n",
    "        # ----------------------------------------\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ·ï¸  [{group}]\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"ì¸êµ¬ ìš”ì•½: í‰ê·  {avg_age:.1f}ì„¸ / {edu_str} {gender_str}\")\n",
    "        print(f\"â€» ì§„ë‹¨ ìš”ì•½: ì·¨ì—…{emp_rate:.1f}% | í™˜ê²½ìœ„í—˜{risk_avg:.1f}% | ì¬ì—°ê²°{reconnect_avg:.2f}\")\n",
    "        print(f\"â€»  ì§„ì„± ì‰¬ì—ˆìŒ ë¹„ìœ¨: {target_rate:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a592e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 6: ìµœì¢… ì„±ëŠ¥ í‰ê°€ + ì‹œë®¬ë ˆì´ì…˜ (Lift, ì»¤ë²„ë¦¬ì§€)\n",
    "\n",
    "def evaluate_total_matrix_performance(df):\n",
    "    \"\"\"\n",
    "    ëª©ì \n",
    "    - ì·¨ì—… ëª¨ë¸ ì„±ëŠ¥(AUC) í™•ì¸\n",
    "    - ì •ì±…êµ°ë³„ 'ì§„ì„±ì‰¬ì—ˆìŒ' í¬ì°©ë¥ ì„ í†µí•´ Lift ì¸¡ì •\n",
    "    - ì¬ì—°ê²°ì§€ìˆ˜ê°€ ì‹¤ì œ ì·¨ì—…ê³¼ ì—°ê´€ë˜ëŠ”ì§€(ì‹ ë¢°ë„) ì ê²€\n",
    "    - ì •ì±…êµ° ì»¤ë²„ë¦¬ì§€(ì •ì±… ìì› ë°°ë¶„ ê·œëª¨) í™•ì¸\n",
    "    - ê°„ë‹¨ ì˜ˆì‚° ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜(íš¨ìœ¨ ë¹„êµ) ì œê³µ\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸš€ [ìµœì¢… ì„±ëŠ¥ í‰ê°€]\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 1. ì·¨ì—…í™•ë¥  ëª¨ë¸ AUC\n",
    "    # ----------------------------------------\n",
    "    y_true = df[\"ì·¨ì—…ì—¬ë¶€\"]\n",
    "    auc = roc_auc_score(y_true, df[\"ì·¨ì—…ì˜ˆì¸¡í™•ë¥ \"])\n",
    "    print(f\"\\n1. ì·¨ì—… ëª¨ë¸ AUC: {auc:.4f}\")\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 2. ì •ì±…ìœ í˜•ë³„ ì§„ì„±ì‰¬ì—ˆìŒ í‰ê· /ì¸ì›\n",
    "    # ----------------------------------------\n",
    "    segment_stats = df.groupby(\"ì •ì±…ìœ í˜•\")[\"ì§„ì„±ì‰¬ì—ˆìŒ\"].agg([\"mean\", \"count\"])\n",
    "    segment_stats[\"mean\"] = segment_stats[\"mean\"] * 100\n",
    "    segment_stats = segment_stats.sort_values(\"mean\", ascending=False)\n",
    "\n",
    "    # Lift ê³„ì‚° ëŒ€ìƒ ê·¸ë£¹ (ê°€ì¥ ê³ ìœ„í—˜ íƒ€ê²Ÿ)\n",
    "    target_group = \"ê³ ì°©í˜• ë‹¨ì ˆ ìœ„í—˜êµ°\"\n",
    "\n",
    "    captured_rate = segment_stats.loc[target_group, \"mean\"] if target_group in segment_stats.index else 0\n",
    "    avg_rate = df[\"ì§„ì„±ì‰¬ì—ˆìŒ\"].mean() * 100\n",
    "\n",
    "    # Lift = (íƒ€ê²Ÿêµ° ìœ„í—˜ë¥  / ì „ì²´ í‰ê·  ìœ„í—˜ë¥ )\n",
    "    lift = captured_rate / avg_rate if avg_rate > 0 else 0\n",
    "\n",
    "    print(f\"\\n2. íƒ€ê²ŸíŒ… ì„±ëŠ¥\")\n",
    "    print(f\"   - ì „ì²´ í‰ê· : {avg_rate:.2f}%\")\n",
    "    print(f\"   - ê³ ì°©í˜• ë‹¨ì ˆ ìœ„í—˜êµ°: {captured_rate:.2f}%\")\n",
    "    print(f\"   Lift: {lift:.2f}ë°°\")\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 3. ì¬ì—°ê²°ì§€ìˆ˜ ì‹ ë¢°ë„\n",
    "    # ----------------------------------------\n",
    "    # ì¬ì—°ê²°ì§€ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ì‹¤ì œ ì·¨ì—…ì´ ë†’ì•„ì•¼ ì •ì±… ì ìˆ˜ë¡œ ì˜ë¯¸ê°€ ìˆìŒ\n",
    "    corr = df[[\"ì¬ì—°ê²°ì§€ìˆ˜\", \"ì·¨ì—…ì—¬ë¶€\"]].corr().iloc[0, 1]\n",
    "    print(f\"\\n3. ì¬ì—°ê²° ì§€ìˆ˜ ì‹ ë¢°ë„(ì·¨ì—…ê³¼ ìƒê´€): {corr:.4f}\")\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 4. ì»¤ë²„ë¦¬ì§€ (ì •ì±…êµ°ë³„ êµ¬ì„±ë¹„)\n",
    "    # ----------------------------------------\n",
    "    print(f\"\\n4. ë§¤íŠ¸ë¦­ìŠ¤ ì»¤ë²„ë¦¬ì§€:\")\n",
    "    coverage = df[\"ì •ì±…ìœ í˜•\"].value_counts(normalize=True) * 100\n",
    "    for group, pct in coverage.items():\n",
    "        print(f\"   - {group}: {pct:.1f}%\")\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 5. ì •ì±…êµ°ë³„ ì§„ì„±ì‰¬ì—ˆìŒ ë¹„ìœ¨ ìƒì„¸\n",
    "    # ----------------------------------------\n",
    "    print(f\"\\n5. ì •ì±… ìœ í˜•ë³„ ì§„ì„±ì‰¬ì—ˆìŒ ë¹„ìœ¨:\")\n",
    "    for group in segment_stats.index:\n",
    "        rate = segment_stats.loc[group, \"mean\"]\n",
    "        count = segment_stats.loc[group, \"count\"]\n",
    "        print(f\"   - {group}: {rate:.2f}% ({int(count)}ëª…)\")\n",
    "\n",
    "    # ==================================================\n",
    "    # ì •ì±… ì‹œë®¬ë ˆì´ì…˜ ê·¸ë˜í”„ (ì˜ˆì‚° ê°€ì¤‘ì¹˜ ê¸°ë°˜)\n",
    "    # ==================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"â€» [ì •ì±… ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ì‹œê°í™”]\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # ì˜ˆì‚° ê°€ì¤‘ì¹˜(ì˜ˆì‹œ)\n",
    "    # ----------------------------------------\n",
    "    \"\"\"\n",
    "    ì˜ë¯¸\n",
    "    - ì‹¤ì œ ì˜ˆì‚°ì´ ì•„ë‹ˆë¼ 'ìƒëŒ€ì  ìì› íˆ¬ì… ê°•ë„'ë¥¼ ê°€ì •í•œ ì‹œë®¬ë ˆì´ì…˜\n",
    "    - ê³ ì°©í˜•ì€ ì§‘ì¤‘ íˆ¬ì…(100), ì•ˆì •êµ°ì€ ìµœì†Œ(10) ê°™ì€ í˜•íƒœ\n",
    "    - ì •ì±… ë…¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜ìš©\n",
    "    \"\"\"\n",
    "    policy_budget = {\n",
    "        \"ê³ ì°©í˜• ë‹¨ì ˆ ìœ„í—˜êµ°\": 100,\n",
    "        \"ì¬ì—°ê²° ê°€ëŠ¥ ë‹¨ì ˆêµ°\": 70,\n",
    "        \"ì·¨ì—…ì·¨ì•½ ë¹„ë‹¨ì ˆêµ°\": 50,\n",
    "        \"ì•ˆì •êµ°\": 10\n",
    "    }\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # íš¨ê³¼ì„±(effectiveness) ê³„ì‚°\n",
    "    # ----------------------------------------\n",
    "    \"\"\"\n",
    "    íš¨ê³¼ì„± = (ì§„ì„±ì‰¬ì—ˆìŒ ë¹„ìœ¨ Ã— ëŒ€ìƒ ì¸ì› Ã— ì˜ˆì‚°ê°€ì¤‘ì¹˜)\n",
    "    - ì—¬ê¸°ì„œ ì§„ì„±ì‰¬ì—ˆìŒ ë¹„ìœ¨ì„ 'ì§€ì› í•„ìš”ë„'ì˜ ëŒ€ë¦¬ë³€ìˆ˜ë¡œ ë³´ê³ \n",
    "    - ì˜ˆì‚°ê°€ì¤‘ì¹˜ë¡œ ì •ì±…ì˜ â€œê°•ë„â€ë¥¼ ë°˜ì˜í•œ ì¢…í•© ì ìˆ˜\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for group in segment_stats.index:\n",
    "        rate = segment_stats.loc[group, \"mean\"]\n",
    "        count = segment_stats.loc[group, \"count\"]\n",
    "        budget = policy_budget.get(group, 0)\n",
    "\n",
    "        effectiveness = (rate / 100) * count * (budget / 100)\n",
    "\n",
    "        results.append({\n",
    "            \"ì •ì±…ìœ í˜•\": group,\n",
    "            \"ìœ„í—˜ë¹„ìœ¨\": rate,\n",
    "            \"ëŒ€ìƒì¸ì›\": count,\n",
    "            \"ì˜ˆì‚°ê°€ì¤‘ì¹˜\": budget,\n",
    "            \"íš¨ê³¼ì„±\": effectiveness\n",
    "        })\n",
    "\n",
    "    df_results = pd.DataFrame(results).sort_values(\"íš¨ê³¼ì„±\", ascending=False)\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # ê·¸ë˜í”„ 1: 2x2 íŒ¨ë„ (ì›ë³¸ ì½”ë“œ ê·¸ëŒ€ë¡œ)\n",
    "    # ----------------------------------------\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1) ìœ„í—˜ë¹„ìœ¨\n",
    "    ax1 = axes[0, 0]\n",
    "    colors1 = ['#e74c3c' if x > 10 else '#3498db' if x > 5 else '#95a5a6' for x in df_results[\"ìœ„í—˜ë¹„ìœ¨\"]]\n",
    "    ax1.barh(df_results[\"ì •ì±…ìœ í˜•\"], df_results[\"ìœ„í—˜ë¹„ìœ¨\"], color=colors1)\n",
    "    ax1.axvline(x=avg_rate, color='red', linestyle='--', linewidth=2, label=f'ì „ì²´í‰ê·  {avg_rate:.1f}%')\n",
    "    ax1.set_xlabel(\"ì§„ì„± ì‰¬ì—ˆìŒ ë¹„ìœ¨ (%)\")\n",
    "    ax1.set_title(\"ì •ì±… ìœ í˜•ë³„ ìœ„í—˜ë„\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    # 2) ëŒ€ìƒì¸ì›\n",
    "    ax2 = axes[0, 1]\n",
    "    colors2 = ['#9b59b6', '#3498db', '#e67e22', '#95a5a6']\n",
    "    ax2.barh(df_results[\"ì •ì±…ìœ í˜•\"], df_results[\"ëŒ€ìƒì¸ì›\"], color=colors2)\n",
    "    ax2.set_xlabel(\"ëŒ€ìƒ ì¸ì› (ëª…)\")\n",
    "    ax2.set_title(\"ì •ì±… ìœ í˜•ë³„ ê·œëª¨\")\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    # 3) ì˜ˆì‚°ê°€ì¤‘ì¹˜\n",
    "    ax3 = axes[1, 0]\n",
    "    colors3 = ['#e74c3c', '#f39c12', '#f1c40f', '#95a5a6']\n",
    "    ax3.barh(df_results[\"ì •ì±…ìœ í˜•\"], df_results[\"ì˜ˆì‚°ê°€ì¤‘ì¹˜\"], color=colors3)\n",
    "    ax3.set_xlabel(\"ì˜ˆì‚° ê°€ì¤‘ì¹˜\")\n",
    "    ax3.set_title(\"ì •ì±…ë³„ íˆ¬ì… ì˜ˆì‚° (ìƒëŒ€ê°’)\")\n",
    "    ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    # 4) ì¢…í•© íš¨ê³¼ì„±\n",
    "    ax4 = axes[1, 1]\n",
    "    colors4 = ['#27ae60' if i == 0 else '#95a5a6' for i in range(len(df_results))]\n",
    "    ax4.barh(df_results[\"ì •ì±…ìœ í˜•\"], df_results[\"íš¨ê³¼ì„±\"], color=colors4)\n",
    "    ax4.set_xlabel(\"ì •ì±… íš¨ê³¼ì„± (ì¢…í•© ì ìˆ˜)\")\n",
    "    ax4.set_title(\"ì •ì±…ë³„ íˆ¬ì… ëŒ€ë¹„ íš¨ê³¼\")\n",
    "    ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{MODEL_JSON_DIR}/policy_simulation_results.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # ê·¸ë˜í”„ 2: Lift ë¹„êµ\n",
    "    # ----------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    lifts = []\n",
    "    for group in segment_stats.index:\n",
    "        rate = segment_stats.loc[group, \"mean\"]\n",
    "        group_lift = rate / avg_rate if avg_rate > 0 else 0\n",
    "        lifts.append({\"ì •ì±…ìœ í˜•\": group, \"Lift\": group_lift})\n",
    "\n",
    "    df_lifts = pd.DataFrame(lifts).sort_values(\"Lift\", ascending=False)\n",
    "\n",
    "    colors_lift = ['#e74c3c' if x > 2 else '#f39c12' if x > 1 else '#95a5a6' for x in df_lifts[\"Lift\"]]\n",
    "    ax.barh(df_lifts[\"ì •ì±…ìœ í˜•\"], df_lifts[\"Lift\"], color=colors_lift)\n",
    "    ax.axvline(x=1.0, color='black', linestyle='--', linewidth=2, label='ê¸°ì¤€ì„  (1.0)')\n",
    "    ax.axvline(x=2.0, color='red', linestyle='--', linewidth=1.5, alpha=0.5, label='ëª©í‘œ (2.0)')\n",
    "    ax.set_xlabel(\"Lift (ë°°ìˆ˜)\")\n",
    "    ax.set_title(\"ì •ì±… ìœ í˜•ë³„ íƒ€ê²ŸíŒ… Lift\")\n",
    "    ax.legend()\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    # ê°’ í‘œì‹œ(ë§‰ëŒ€ ì˜†ì— í…ìŠ¤íŠ¸)\n",
    "    for i, (idx, row) in enumerate(df_lifts.iterrows()):\n",
    "        ax.text(row[\"Lift\"] + 0.05, i, f'{row[\"Lift\"]:.2f}ë°°', va='center', fontsize=10, weight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{MODEL_JSON_DIR}/policy_lift_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"ì •ì±… ì‹œë®¬ë ˆì´ì…˜ ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ\")\n",
    "\n",
    "    # ==================================================\n",
    "    # â€» ì•„ë˜ ë¸”ë¡ì€ ì›ë³¸ ì½”ë“œì— \"ë™ì¼ í‰ê°€ ì¶œë ¥ì´ í•œ ë²ˆ ë”\" ì¡´ì¬\n",
    "    # (ìš”ì²­ëŒ€ë¡œ ì‚­ì œí•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë‘ë˜, ì˜ë¯¸ë¥¼ ì£¼ì„ìœ¼ë¡œ ì„¤ëª…)\n",
    "    # ==================================================\n",
    "    \"\"\"\n",
    "    ì™œ ì¤‘ë³µ ì¶œë ¥ì´ ìˆë‚˜?\n",
    "    - ê°œë°œ ì¤‘ê°„ì— ì‹œë®¬ë ˆì´ì…˜ ì¶”ê°€í•˜ë©´ì„œ\n",
    "      ê¸°ì¡´ í‰ê°€ ì¶œë ¥ ë¸”ë¡ì„ ì•„ë˜ì— ë‹¤ì‹œ ë¶™ì—¬ ë„£ì€ í˜•íƒœë¡œ ë³´ì„\n",
    "    - ê¸°ëŠ¥ìƒ ë¬¸ì œëŠ” ì—†ì§€ë§Œ ì¶œë ¥ì´ 2ë²ˆ ë°˜ë³µë¨\n",
    "    - ë‚˜ì¤‘ì— ì •ë¦¬í•  ë• ì•„ë˜ ì¤‘ë³µ ë¸”ë¡ì„ ì œê±°í•˜ë©´ ê¹”ë”í•´ì§\n",
    "      (í•˜ì§€ë§Œ í˜„ì¬ëŠ” 'ì‚­ì œ ê¸ˆì§€' ì¡°ê±´ ë•Œë¬¸ì— ìœ ì§€)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸš€ [ìµœì¢… ì„±ëŠ¥ í‰ê°€]\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    y_true = df[\"ì·¨ì—…ì—¬ë¶€\"]\n",
    "    auc = roc_auc_score(y_true, df[\"ì·¨ì—…ì˜ˆì¸¡í™•ë¥ \"])\n",
    "    print(f\"\\n1. ì·¨ì—… ëª¨ë¸ AUC: {auc:.4f}\")\n",
    "\n",
    "    segment_stats = df.groupby(\"ì •ì±…ìœ í˜•\")[\"ì§„ì„±ì‰¬ì—ˆìŒ\"].agg([\"mean\", \"count\"])\n",
    "    segment_stats[\"mean\"] = segment_stats[\"mean\"] * 100\n",
    "    segment_stats = segment_stats.sort_values(\"mean\", ascending=False)\n",
    "\n",
    "    target_group = \"ê³ ì°©í˜• ë‹¨ì ˆ ìœ„í—˜êµ°\"\n",
    "    captured_rate = segment_stats.loc[target_group, \"mean\"] if target_group in segment_stats.index else 0\n",
    "    avg_rate = df[\"ì§„ì„±ì‰¬ì—ˆìŒ\"].mean() * 100\n",
    "    lift = captured_rate / avg_rate if avg_rate > 0 else 0\n",
    "\n",
    "    print(f\"\\n2. íƒ€ê²ŸíŒ… ì„±ëŠ¥\")\n",
    "    print(f\"   - ì „ì²´ í‰ê· : {avg_rate:.2f}%\")\n",
    "    print(f\"   - ê³ ì°©í˜• ë‹¨ì ˆ ìœ„í—˜êµ°: {captured_rate:.2f}%\")\n",
    "    print(f\"   Lift: {lift:.2f}ë°°\")\n",
    "\n",
    "    corr = df[[\"ì¬ì—°ê²°ì§€ìˆ˜\", \"ì·¨ì—…ì—¬ë¶€\"]].corr().iloc[0, 1]\n",
    "    print(f\"\\n3. ì¬ì—°ê²° ì§€ìˆ˜ ì‹ ë¢°ë„: {corr:.4f}\")\n",
    "\n",
    "    print(f\"\\n4. ë§¤íŠ¸ë¦­ìŠ¤ ì»¤ë²„ë¦¬ì§€:\")\n",
    "    coverage = df[\"ì •ì±…ìœ í˜•\"].value_counts(normalize=True) * 100\n",
    "    for group, pct in coverage.items():\n",
    "        print(f\"   - {group}: {pct:.1f}%\")\n",
    "\n",
    "    print(f\"\\n5. ì •ì±… ìœ í˜•ë³„ ì§„ì„±ì‰¬ì—ˆìŒ ë¹„ìœ¨:\")\n",
    "    for group in segment_stats.index:\n",
    "        rate = segment_stats.loc[group, \"mean\"]\n",
    "        count = segment_stats.loc[group, \"count\"]\n",
    "        print(f\"   - {group}: {rate:.2f}% ({int(count)}ëª…)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a924e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN ì‹¤í–‰ë¶€: ì „ì²´ íŒŒì´í”„ë¼ì¸ (ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜)\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸš€ FINAL COMPLETE PIPELINE START\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ======================================================\n",
    "# STEP 0: ì²­ë…„ ë¯¸ì‹œ ë°ì´í„° ë¡œë“œ + ìƒê´€ê´€ê³„ ë¶„ì„\n",
    "# ======================================================\n",
    "\"\"\"\n",
    "ì—¬ê¸°ì„œ í•˜ëŠ” ì¼\n",
    "1) youth_may(ê°œì¸ ë‹¨ìœ„) ë¡œë“œ\n",
    "2) ì§„ì„±ì‰¬ì—ˆìŒ ë¼ë²¨ ìƒì„±\n",
    "3) íƒ€ê²Ÿê³¼ ì—°ê´€ëœ ë³€ìˆ˜ íƒìƒ‰\n",
    "4) ì •ë³´ ëˆ„ìˆ˜ í•„í„°ë§ í›„, ì•ˆì „í•œ ì¶”ê°€ ë³€ìˆ˜ í›„ë³´(high_corr_vars) ìƒì„±\n",
    "\n",
    "high_corr_varsëŠ” PART 2 ì·¨ì—… ëª¨ë¸ì— \"ì¶”ê°€ ë³€ìˆ˜ í›„ë³´\"ë¡œë§Œ ì‚¬ìš©ë¨\n",
    "(ì¦‰, ìƒê´€ë¶„ì„ì´ ê³§ ë³€ìˆ˜ í™•ì •ì´ ì•„ë‹ˆë¼ 'í›„ë³´ í’€' ìƒì„± ë‹¨ê³„)\n",
    "\"\"\"\n",
    "df_full = load_youth_may_full()\n",
    "corr_matrix, high_corr_vars = analyze_full_correlation(df_full)\n",
    "\n",
    "# ======================================================\n",
    "# STEP 1: ë‹¨ì ˆìœ„í—˜ ëª¨ë¸ í•™ìŠµ (í™˜ê²½ ìœ„í—˜)\n",
    "# ======================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: ë‹¨ì ˆìœ„í—˜ ëª¨ë¸ í•™ìŠµ (GBDT Enhanced)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\"\"\"\n",
    "ì—¬ê¸°ì„œ í•˜ëŠ” ì¼\n",
    "- eaps_year(ì—°ê°„) ë°ì´í„°ë¡œ\n",
    "  'í™˜ê²½ ìœ„í—˜(êµ¬ì¡°ì  ë‹¨ì ˆ)' ëª¨ë¸ì„ í•™ìŠµí•œë‹¤.\n",
    "\n",
    "ê²°ê³¼:\n",
    "- df_mainì— ê°œì¸ë³„ ë‹¨ì ˆìœ„í—˜í™•ë¥ ì´ ë¶™ê³ \n",
    "- ì´í›„ KEY(ì—°ì›”Ã—ì„±ë³„Ã—ì—°ë ¹ëŒ€)ë¡œ í‰ê· ë‚´ì–´ í™˜ê²½ìœ„í—˜ì„ ë§Œë“ ë‹¤.\n",
    "\"\"\"\n",
    "df_main = load_eaps_year()\n",
    "df_main = add_derived_features(df_main)\n",
    "(\n",
    "    disconnect_model,\n",
    "    Xd_train, yd_train,\n",
    "    Xd_test, yd_test,\n",
    "    df_main\n",
    ") = run_disconnect_model_enhanced(df_main)\n",
    "\n",
    "# ======================================================\n",
    "# STEP 2: ì·¨ì—…í™•ë¥  ëª¨ë¸ í•™ìŠµ (ê°œì¸ ì·¨ì—…í™•ë¥ )\n",
    "# ======================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: ì·¨ì—…í™•ë¥  ëª¨ë¸ í•™ìŠµ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\"\"\"\n",
    "ì—¬ê¸°ì„œ í•˜ëŠ” ì¼\n",
    "- df_full(ê°œì¸ ë‹¨ìœ„)ë¡œ ì·¨ì—…í™•ë¥ ì„ í•™ìŠµí•œë‹¤.\n",
    "- Calibration(isotonic) ì ìš©ìœ¼ë¡œ ì˜ˆì¸¡í™•ë¥ ì´ ì •ì±…ì ìœ¼ë¡œ í•´ì„ ê°€ëŠ¥í•´ì§„ë‹¤.\n",
    "- additional_features=high_corr_varsë¡œ\n",
    "  'ì•ˆì „ ë³€ìˆ˜ í›„ë³´' ì¼ë¶€ë¥¼ ì¶”ê°€í•´ ì„±ëŠ¥/ì„¤ëª…ë ¥ì„ ë³´ì™„í•œë‹¤.\n",
    "\"\"\"\n",
    "(\n",
    "    employment_model,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    df_test\n",
    ")= train_employment_gbdt_enhanced(\n",
    "    df_full, additional_features=high_corr_vars\n",
    ")\n",
    "\n",
    "# ======================================================\n",
    "# STEP 3: ì •ì±… ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì„± (í™˜ê²½Ã—ê°œì¸ ê²°í•©)\n",
    "# ======================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: ì •ì±… ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì„±\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\"\"\"\n",
    "ì—¬ê¸°ì„œ í•˜ëŠ” ì¼\n",
    "- df_mainì—ì„œ í™˜ê²½ìœ„í—˜(ë‹¨ì ˆìœ„í—˜_í™˜ê²½í‰ê· ) ìƒì„±\n",
    "- df_test(ê°œì¸ ì·¨ì—…í™•ë¥ )ì— merge\n",
    "- ì—°ë ¹ëŒ€ë³„ ë¶„ìœ„ìˆ˜ ì»·ìœ¼ë¡œ ì •ì±…ìœ í˜•ì„ ë¶„ë¥˜\n",
    "- ì¬ì—°ê²°ì§€ìˆ˜ê¹Œì§€ ê³„ì‚°í•˜ì—¬ ë‹¨ì¼ ìŠ¤ì½”ì–´ ì œê³µ\n",
    "\"\"\"\n",
    "df_policy, risk_env = build_hybrid_policy_matrix(df_main, df_test)\n",
    "\n",
    "print(\"\\n[ì •ì±…ìœ í˜• ë¶„í¬]\")\n",
    "print(df_policy[\"ì •ì±…ìœ í˜•\"].value_counts())\n",
    "\n",
    "print(\"\\n[ì—°ë ¹ëŒ€ë³„ ë¶„í¬]\")\n",
    "print(pd.crosstab(df_policy[\"ì—°ë ¹ëŒ€\"], df_policy[\"ì •ì±…ìœ í˜•\"]))\n",
    "\n",
    "# ======================================================\n",
    "# STEP 4: EDA ì‹œê°í™”\n",
    "# ======================================================\n",
    "\"\"\"\n",
    "ì •ì±… ë³´ê³ ì„œìš© ê·¸ë¦¼ ìë™ ìƒì„±\n",
    "- ridge_plot_final.png\n",
    "- cluster_scatter.png\n",
    "- boxplot_reconnect.png\n",
    "- targeting_accuracy.png\n",
    "\"\"\"\n",
    "plot_eda_visualizations(df_policy)\n",
    "\n",
    "# ======================================================\n",
    "# STEP 5: í˜ë¥´ì†Œë‚˜ ìƒì„±\n",
    "# ======================================================\n",
    "\"\"\"\n",
    "ì •ì±…ìœ í˜•ë³„ ëŒ€í‘œ ì†ì„± ìš”ì•½\n",
    "- í‰ê·  ì—°ë ¹, ëŒ€í‘œ ì„±ë³„/í•™ë ¥\n",
    "- ì·¨ì—…ë¥ , ìœ„í—˜ë„, ì¬ì—°ê²°ì§€ìˆ˜, ì§„ì„±ì‰¬ì—ˆìŒ ë¹„ìœ¨\n",
    "\"\"\"\n",
    "generate_personas(df_policy)\n",
    "\n",
    "# ======================================================\n",
    "# STEP 6: ìµœì¢… ì„±ëŠ¥ í‰ê°€ + ì‹œë®¬ë ˆì´ì…˜\n",
    "# ======================================================\n",
    "\"\"\"\n",
    "ì •ì±…ëª¨í˜•ì˜ ëª©ì ì€ ë‹¨ìˆœ AUCê°€ ì•„ë‹ˆë¼\n",
    "- íƒ€ê²ŸíŒ… Lift\n",
    "- ì»¤ë²„ë¦¬ì§€\n",
    "- ì¬ì—°ê²°ì§€ìˆ˜ì˜ ì •ì±…ì  ì‹ ë¢°ë„\n",
    "ë¥¼ í•¨ê»˜ ë³´ëŠ” ê²ƒ\n",
    "\"\"\"\n",
    "evaluate_total_matrix_performance(df_policy)\n",
    "\n",
    "# ======================================================\n",
    "# ê²°ê³¼ ì €ì¥ (ëª¨ë¸/ë°ì´í„°)\n",
    "# ======================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ’¾ ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# êµ¬ì¡°ì  ë‹¨ì ˆ ìœ„í—˜ ëª¨ë¸ ì €ì¥\n",
    "joblib.dump(disconnect_model, f\"{MODEL_DIR}/disconnect_gbdt_final.pkl\")\n",
    "\n",
    "# ì·¨ì—…í™•ë¥  ëª¨ë¸ ì €ì¥ (Calibration í¬í•¨ ê°ì²´)\n",
    "joblib.dump(employment_model, f\"{MODEL_DIR}/employment_gbdt_final.pkl\")\n",
    "\n",
    "# ì •ì±… ë§¤íŠ¸ë¦­ìŠ¤ ê²°ê³¼ ì €ì¥\n",
    "df_policy.to_csv(f\"{CSV_DIR}/policy_matrix_final.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ì €ì¥ ì™„ë£Œ!\")\n",
    "print(f\"   - ëª¨ë¸: {MODEL_DIR}\")\n",
    "print(f\"   - ì‹œê°í™”: {MODEL_JSON_DIR}\")\n",
    "print(f\"   - ë°ì´í„°: {CSV_DIR}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbade0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# TEST 1: êµ¬ì¡°ì  ë‹¨ì ˆ ìœ„í—˜ ëª¨ë¸ (Train vs Test)\n",
    "# ======================================================\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def test_disconnect_overfitting(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    êµ¬ì¡°ì  ë‹¨ì ˆ ìœ„í—˜ ëª¨ë¸ ì˜¤ë²„í”¼íŒ… ì ê²€\n",
    "    \"\"\"\n",
    "    # Train\n",
    "    y_train_prob = model.predict_proba(X_train)[:, 1]\n",
    "    train_auc = roc_auc_score(y_train, y_train_prob)\n",
    "\n",
    "    # Test\n",
    "    y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "    test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ§ª [TEST] êµ¬ì¡°ì  ë‹¨ì ˆ ìœ„í—˜ ëª¨ë¸\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Train AUC : {train_auc:.4f}\")\n",
    "    print(f\"Test  AUC : {test_auc:.4f}\")\n",
    "    print(f\"Gap       : {train_auc - test_auc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"train_auc\": train_auc,\n",
    "        \"test_auc\": test_auc,\n",
    "        \"gap\": train_auc - test_auc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_disconnect_overfitting(\n",
    "    disconnect_model,\n",
    "    Xd_train, yd_train,\n",
    "    Xd_test, yd_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee2f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# TEST 2: ì·¨ì—…í™•ë¥  ëª¨ë¸ (Train vs Test)\n",
    "# ======================================================\n",
    "\n",
    "def test_employment_overfitting(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    ì·¨ì—…í™•ë¥  ëª¨ë¸ ì˜¤ë²„í”¼íŒ… ì ê²€\n",
    "    \"\"\"\n",
    "    # Train\n",
    "    y_train_prob = model.predict_proba(X_train)[:, 1]\n",
    "    train_auc = roc_auc_score(y_train, y_train_prob)\n",
    "\n",
    "    # Test\n",
    "    y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "    test_auc = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ§ª [TEST] ì·¨ì—…í™•ë¥  ëª¨ë¸\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Train AUC : {train_auc:.4f}\")\n",
    "    print(f\"Test  AUC : {test_auc:.4f}\")\n",
    "    print(f\"Gap       : {train_auc - test_auc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"train_auc\": train_auc,\n",
    "        \"test_auc\": test_auc,\n",
    "        \"gap\": train_auc - test_auc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_employment_overfitting(\n",
    "    employment_model,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e8315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# TEST 3: Calibration Curve (ì •ì±…ìš© í•„ìˆ˜)\n",
    "# ======================================================\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_calibration_curve(y_true, y_prob, title=\"Calibration Curve\"):\n",
    "    prob_true, prob_pred = calibration_curve(\n",
    "        y_true, y_prob,\n",
    "        n_bins=10,\n",
    "        strategy=\"quantile\"\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label='Model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect')\n",
    "    plt.xlabel(\"ì˜ˆì¸¡ í™•ë¥ \")\n",
    "    plt.ylabel(\"ì‹¤ì œ ì·¨ì—… ë¹„ìœ¨\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df859b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_calibration_curve(\n",
    "    y_test,\n",
    "    employment_model.predict_proba(X_test)[:, 1],\n",
    "    title=\"Employment Model Calibration (Test)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea96751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# TEST 4: ì •ì±… íƒ€ê²ŸíŒ… Lift í…ŒìŠ¤íŠ¸\n",
    "# ======================================================\n",
    "\n",
    "def test_topk_lift(df, k=0.2, score_col=\"ì§€ì›í•„ìš”ë„ì ìˆ˜\"):\n",
    "    \"\"\"\n",
    "    ì§‘ì¤‘ íƒ€ê²ŸíŒ… Lift (Top-K ê¸°ì¤€)\n",
    "    \"\"\"\n",
    "    base_rate = df[\"ì§„ì„±ì‰¬ì—ˆìŒ\"].mean()\n",
    "    n = int(len(df) * k)\n",
    "\n",
    "    top = df.sort_values(score_col, ascending=False).head(n)\n",
    "    top_rate = top[\"ì§„ì„±ì‰¬ì—ˆìŒ\"].mean()\n",
    "    lift = top_rate / base_rate if base_rate > 0 else 0\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ğŸ§ª [TEST] ì§‘ì¤‘ íƒ€ê²ŸíŒ… Lift (Top {int(k*100)}%)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ëŒ€ìƒ ì¸ì›        : {n}\")\n",
    "    print(f\"Top-K ìœ„í—˜ë¹„ìœ¨   : {top_rate*100:.2f}%\")\n",
    "    print(f\"ì „ì²´ í‰ê·         : {base_rate*100:.2f}%\")\n",
    "    print(f\"Lift             : {lift:.2f}ë°°\")\n",
    "\n",
    "    return lift\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fe3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [0.30, 0.20, 0.10]:\n",
    "    test_topk_lift(df_policy, k=k)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
